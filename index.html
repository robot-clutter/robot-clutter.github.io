<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Robot Clutter</title><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/5788f1bca5613114.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5788f1bca5613114.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-0ba0ddd33199226d.js" defer=""></script><script src="/_next/static/chunks/main-f9ea798222b0d70a.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a3c4709e2d9f5c6d.js" defer=""></script><script src="/_next/static/chunks/947-cce2e87fb27c2238.js" defer=""></script><script src="/_next/static/chunks/pages/index-88600ecf85919919.js" defer=""></script><script src="/_next/static/b5jCc4ua792Jz2fEwrqGs/_buildManifest.js" defer=""></script><script src="/_next/static/b5jCc4ua792Jz2fEwrqGs/_ssgManifest.js" defer=""></script></head><body class="bg-gray-50 text-black dark:bg-gray-900 dark:text-white"><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><nav class="bg-gray-50 dark:bg-gray-900 border-b border-gray-200 dark:border-gray-500"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex"><div class="flex-shrink-0 flex items-center"></div></div><div class="ml-6 flex items-center"></div></div></div></nav><div class="text-center"><div class="relative pt-10 px-4 sm:px-6 lg:pt-14 lg:px-8"><div class="absolute inset-0"><div class="bg-gray-50 dark:bg-gray-900 h-1/3 sm:h-2/3"></div></div><div class="relative max-w-7xl mx-auto"><div class="text-center"><h2 class="text-3xl tracking-tight font-extrabold text-gray-900 dark:text-gray-300 sm:text-4xl">Learning-based robotic manipulation in cluttered environments</h2></div><div class="text-justify mt-10 text-xl">In this project, we aim to research and develop learning-based methods for manipulating objects in cluttered environments. During this project a number of publications have been produced and are listed below. Furthermore, the software developed for these methods is open source and can be found in our <span class="mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a target="_blank" rel="noreferrer" href="https://github.com/robot-clutter">github repos</a></span> along with its <span class="mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a target="_blank" rel="noreferrer" href="https://robot-clutter.github.io/clt_core">documentation</a></span>.<p>This research was conducted in <span class="mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a target="_blank" rel="noreferrer" href="https://arl.ee.auth.gr">Automation and Robotics Lab</a></span> of <span class="mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a target="_blank" rel="noreferrer" href="https://www.auth.gr/en/">Aristotle University of Thessaloniki</a></span>.</p></div></div></div><div class="relative pt-10 pb-20 px-4 sm:px-6 lg:pt-14 lg:pb-28 lg:px-8"><div class="absolute inset-0"><div class="bg-gray-50 dark:bg-gray-900 h-1/3 sm:h-2/3"></div></div><div class="relative max-w-7xl mx-auto"><div class="text-center"><h2 class="text-3xl tracking-tight font-extrabold text-gray-900 dark:text-gray-300 sm:text-4xl">Publications</h2></div><div class="mt-12 max-w-lg mx-auto grid gap-5 lg:grid-cols-3 lg:max-w-none"><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><img class="h-48 w-full object-cover cursor-pointer" src="/push_grasping_header_image.png" alt=""/></div><div class="flex-1 bg-gray-50 dark:bg-gray-800 p-6 flex flex-col justify-between"><div class="flex-1"><div class="block mt-2"><p class="text-xl font-semibold text-gray-900 dark:text-gray-300 text-left"><a href="/ppg">Learning Push-Grasping in Dense Clutter</a></p><p class="mt-1 text-base font-semibold text-gray-500 dark:text-gray-400 text-left">IEEE RAL 2022</p><p class="mt-3 text-base text-gray-500 dark:text-gray-400 text-left truncate">Robotic grasping in highly cluttered environments remains a challenging task due to the lack of collision free grasp affordances. In such conditions, non-prehensile actions could help to increase such affordances. We propose a multi-fingered push-grasping policy that creates enough space for the fingers to wrap around an object to perform a stable power grasp, using a single primitive action. Our approach learns a direct mapping from visual observations to actions and is trained in a fully end-to-end manner. To achieve a more efficient learning, we decouple the action space by learning separately the robot hand pose and finger configuration. Experiments in simulation demonstrate that the proposed push-grasping policy achieves higher grasp success rate over baselines and that can generalize to unseen objects. Furthermore, although training is performed in simulation the learned policy is robustly transferred to a real environment without a significant drop in success rate.</p></div><p class="text-left mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a href="/ppg">Read More</a></p></div></div></div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><img class="h-48 w-full object-cover cursor-pointer" src="/bridging_the_gap_header_image.jpg" alt=""/></div><div class="flex-1 bg-gray-50 dark:bg-gray-800 p-6 flex flex-col justify-between"><div class="flex-1"><div class="block mt-2"><p class="text-xl font-semibold text-gray-900 dark:text-gray-300 text-left"><a href="/bridging-the-gap">Bridging the gap between learning and heuristic based pushing policies</a></p><p class="mt-1 text-base font-semibold text-gray-500 dark:text-gray-400 text-left">Preprint 2021</p><p class="mt-3 text-base text-gray-500 dark:text-gray-400 text-left truncate">Non-prehensile pushing actions have the potential to singulate a target object from its surrounding clutter in order to facilitate the robotic grasping of the target. To address this problem we utilize a heuristic rule that moves the target object towards the workspace&#x27;s empty space and demonstrate that this simple heuristic rule achieves singulation. Furthermore, we incorporate this heuristic rule to the reward in order to train more efficiently reinforcement learning (RL) agents for singulation. Simulation experiments demonstrate that this insight increases performance. Finally, our results show that the RL-based policy implicitly learns something similar to one of the used heuristics in terms of decision making.</p></div><p class="text-left mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a href="/bridging-the-gap">Read More</a></p></div></div></div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><img class="h-48 w-full object-cover cursor-pointer" src="/modular_rl_header_image.png" alt=""/></div><div class="flex-1 bg-gray-50 dark:bg-gray-800 p-6 flex flex-col justify-between"><div class="flex-1"><div class="block mt-2"><p class="text-xl font-semibold text-gray-900 dark:text-gray-300 text-left"><a href="/modular-rl">Total Singulation with Modular Reinforcement Learning</a></p><p class="mt-1 text-base font-semibold text-gray-500 dark:text-gray-400 text-left">RAL 2021</p><p class="mt-3 text-base text-gray-500 dark:text-gray-400 text-left truncate">Prehensile robotic grasping of a target object in clutter is challenging because, in such conditions, the target touches other objects, resulting to the lack of collision free grasp affordances. To address this problem, we propose a modular reinforcement learning method which uses continuous actions to totally singulate the target object from its surrounding clutter. A high level policy selects between pushing primitives, which are learned separately. Prior knowledge is effectively incorporated into learning, through action primitives and feature selection increasing sample efficiency. Experiments demonstrate that the proposed method considerably outperforms the state-of-the-art methods in the singulation task. Furthermore, although training is performed in simulation the learned policy is robustly transferred to a real environment without a significant drop in success rate. Finally, singulation tasks in different environments are addressed by easily adding a new primitive and by retraining only the high level policy.</p></div><p class="text-left mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a href="/modular-rl">Read More</a></p></div></div></div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><img class="h-48 w-full object-cover cursor-pointer" src="/split_dqn_header_image.jpg" alt=""/></div><div class="flex-1 bg-gray-50 dark:bg-gray-800 p-6 flex flex-col justify-between"><div class="flex-1"><div class="block mt-2"><p class="text-xl font-semibold text-gray-900 dark:text-gray-300 text-left"><a href="/split-dqn">Split Deep Q-Learning for Robust Object Singulation</a></p><p class="mt-1 text-base font-semibold text-gray-500 dark:text-gray-400 text-left">ICRA 2020</p><p class="mt-3 text-base text-gray-500 dark:text-gray-400 text-left truncate">Extracting a known target object from a pile of other objects in a cluttered environment is a challenging robotic manipulation task encountered in many robotic applications. In such conditions, the target object touches or is covered by adjacent obstacle objects, thus rendering traditional grasping techniques ineffective. In this paper, we propose a pushing policy aiming at singulating the target object from its surrounding clutter, by means of lateral pushing movements of both the neighboring objects and the target object until sufficient &#x27;grasping room&#x27; has been achieved. To achieve the above goal we employ reinforcement learning and particularly Deep Q-learning (DQN) to learn optimal push policies by trial and error. A novel Split DQN is proposed to improve the learning rate and increase the modularity of the algorithm. Experiments show that although learning is performed in a simulated environment the transfer of learned policies to a real environment is effective thanks to robust feature selection. Finally, we demonstrate that the modularity of the algorithm allows the addition of extra primitives without retraining the model from scratch.</p></div><p class="text-left mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a href="/split-dqn">Read More</a></p></div></div></div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><img class="h-48 w-full object-cover cursor-pointer" src="/dqn_header_image.jpg" alt=""/></div><div class="flex-1 bg-gray-50 dark:bg-gray-800 p-6 flex flex-col justify-between"><div class="flex-1"><div class="block mt-2"><p class="text-xl font-semibold text-gray-900 dark:text-gray-300 text-left"><a href="/dqn">Robust object grasping in clutter via singulation</a></p><p class="mt-1 text-base font-semibold text-gray-500 dark:text-gray-400 text-left">ICRA 2019</p><p class="mt-3 text-base text-gray-500 dark:text-gray-400 text-left truncate">Grasping objects in a cluttered environment is challenging due to the lack of collision free grasp affordances. In such conditions, the target object touches or is covered by other objects in the scene, resulting in a failed grasp. To address this problem, we propose a strategy of singulating the object from its surrounding clutter, which consists of previously unseen objects, by means of lateral pushing movements. We employ reinforcement learning for obtaining optimal push policies given depth observations of the scene. The action-value function(Q-function) is approximated with a deep neural network. We train the robot in simulation and we demonstrate that the transfer of learned policies to the real environment is robust.</p></div><p class="text-left mt-6 font-medium text-cyan-600 dark:text-cyan-500 hover:underline"><a href="/dqn">Read More</a></p></div></div></div></div></div></div></div><style data-emotion-css="xg5s2c">.css-xg5s2c{box-sizing:border-box;max-height:100%;max-width:100%;overflow:hidden;padding:8px;pointer-events:none;position:fixed;z-index:1000;top:0;right:0;}</style><div class="react-toast-notifications__container css-xg5s2c"></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/","query":{},"buildId":"b5jCc4ua792Jz2fEwrqGs","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>